{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA(p,q)\n",
    "\n",
    "ARMA(p,q):= is a combination of AR(p) ie autocorrection model, a model that predicts future values based on past values and their white noise, and MA(q) ie moving average that predicts future values based on past errors.\n",
    "\n",
    "## AR(p)\n",
    "\n",
    "AR(p) intuitivly lets create an AR model first. AR:= a model that aims to express a time series as a function of its past values; $$X_t = \\beta_0 + \\beta_1 X_{t-1} + \\beta_2 X_{t-2} + \\cdots + \\beta_p X_{t-p} + \\varepsilon_t,\n",
    "\\qquad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$$.\n",
    "*epsilon is a white noise. \n",
    "Stationarity:= a process is weakly stationary iff; $$E(X)=µ, Var(X)=\\sigma^2, Cov(X_{t}, X_{t-h})$$ is dependent on h not t. For AR(1) stationary iff |β|<1, for AR(p) stationary iff $$ β(z) = 1 - β1 z - β2 z^2 - β3 z^3 ... =0$$ are not solutions dont lie in the unit circle, i.e. $| \\beta_i |<1$.\n",
    "\n",
    "estimating the coefficients of β we have a few techniques;\n",
    "\n",
    "### 1. OLS, \n",
    " we can write it into matrix for; $$ Y = \\beta X + \\varepsilon $$.\n",
    " OLS estimator (minimising square error) := $$ \\min_{\\beta} \\ \\hat{\\varepsilon}'\\hat{\\varepsilon}\n",
    " = (Y - X\\beta)'(Y - X\\beta)\n",
    " = Y'Y - Y'X\\beta - \\beta'X'Y + \\beta'X'X\\beta. $$ due to it being a symetric matrix then; $$ \\min_{\\beta} \\ \\hat{\\varepsilon}'\\hat{\\varepsilon}=Y'Y -2β'X'Y +β'X'Xβ$$ \n",
    " $$FOC: \\frac{\\partial}{\\partial \\beta} = 0\n",
    " = -2X'Y + 2X'X\\beta$$\n",
    " $$ \\qquad\\Rightarrow\\qquad  \\widehat{\\beta} = (X'X)^{-1}X'Y. \n",
    "  $$\n",
    "  Lets label this (ARE 1) for when we can compare pros; doesnt require stationarity, cons; No autocorrection optimality, endogeniety breaks model.\n",
    "\n",
    "### 2. Yule Walker,\n",
    " need to assume stationary, let; $$ɣ(h)= Cov(X_t, X_{t-h})$$, note; $$β_0 E( X_t X_{t-k} ) = E(β_1 X_{t-1} X_{t-k} + β_2 X_{t-2} X_{t-k} +...+ β_p X_{t-p} X_{t-k} + ℇ_t X_{t-k}) = β_1 E(X_{t-k} X_{t-1}) +...+β_p E(X_{t-p} X_{t-k}), E(ℇ_t X_{t-k})=0$$ as white noise uncorrelated to past values => $$ɣ(k)= β_1 ɣ(k-1) + β_2 ɣ(k-2) +...+ β_p ɣ(k-p) let β = (β_1, β_2,..., β_p)' , ɣ_p = (ɣ(1), ɣ(2),..., ɣ(p))',$$ $$Γ_p = \\begin{pmatrix}\n",
    "\\gamma(0) & \\gamma(1) & \\cdots & \\gamma(p-1) \\\\\n",
    "\\gamma(1) & \\gamma(0) & \\cdots & \\gamma(p-2) \\\\\n",
    "\\gamma(2) & \\gamma(1) & \\cdots & \\gamma(p-3) \\\\\n",
    "\\vdots    & \\vdots    & \\ddots & \\vdots      \\\\\n",
    "\\gamma(p-1) & \\gamma(p-2) & \\cdots & \\gamma(0)\n",
    "\\end{pmatrix} $$you've seen a covarience matrix before.\n",
    "Then $Γ_p \\hat{β} = ɣ_p$ or $ \\hat{β} = {Γ_p}^{-1} ɣ_p $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Extra reading: going back to why if its in the roots are in the unit circle its not stationary just because its interesting: lets add a lag opperator L such that $ L X_t = X_{t-1}, L X_{t-1} = X_{t-2}$ or $ L^2 X_t = X_{t-2}$ then; $$ X_t = β_1 X_{t-1} + β_2 X_{t-2} +...+ β_p X_{t-p} +ℇ_t, $$ becomes; $$ X_t = X_t{ β_1 L + β_2 L^2 +...+ L^p} +ℇ_t, $$ or moving to one side $$ ℇ_t = X_t{ 1 - β_1 L - β_2 L^2 -...- L^p}$$, compact this; $$ ℇ_t = β(L) X_t => X_t = β(L)^{-1} ℇ_t,$$ which is an infinite sum that will only converge if past shocks have diminishing effect which wont happen with roots within the unit circle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r9/p1cb21jx0gx9cpxjwsglndy80000gn/T/ipykernel_84322/3363888553.py:13: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download('^GSPC', start='2020-01-01', end='2025-06-15', interval='1d')\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated AR coefficients (beta_hat): [ 0.99749373 -0.00250627]\n",
      "Estimated noise variance (sigma^2): [3.30042991e-26]\n",
      "Roots of the characteristic polynomial: [0.99497481 0.00251892]\n",
      "Absolute values of roots: [0.99497481 0.00251892]\n",
      "The AR process is NOT stationary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#AR(p); autoregressive\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import yfinance as yf #packages\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Parameters\n",
    "T = 200 #how many past data points you will use \n",
    "p = 2 #amount of beta terms\n",
    "\n",
    "#Get data\n",
    "data = yf.download('^GSPC', start='2020-01-01', end='2025-06-15', interval='1d')\n",
    "X = data['Close'].tail(T).values  # convert to numpy array\n",
    "X -= X - X.mean()\n",
    "\n",
    "def sam_AC(X, h):  # Sample autocovariance function\n",
    "    T = len(X)\n",
    "    Xbar = np.mean(X)\n",
    "    cov = 0\n",
    "    for t in range(h, T):\n",
    "        cov += (X[t] - Xbar) * (X[t - h] - Xbar)\n",
    "    return cov / T\n",
    "\n",
    "def toeplitz(c):\n",
    "    n = len(c)\n",
    "    TP = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            TP[i, j] = c[abs(i - j)].item()\n",
    "    return TP\n",
    "\n",
    "def YW(X, p):  # Yule-Walker Estimation\n",
    "    gamma = np.array([sam_AC(X, h) for h in range(p+1)])\n",
    "    Gamma_p = toeplitz(gamma[:-1])\n",
    "    gamma_p = gamma[1:]\n",
    "    beta_hat = np.linalg.solve(Gamma_p, gamma_p)\n",
    "    \n",
    "    # Ensure both are 1D arrays before the dot product\n",
    "    beta_hat = beta_hat.flatten()\n",
    "    gamma_p = gamma_p.flatten()\n",
    "    \n",
    "    sigma2 = gamma[0] - np.dot(beta_hat, gamma_p)\n",
    "    return beta_hat, sigma2\n",
    "\n",
    "# Run estimation\n",
    "beta_hat, sigma2 = YW(X, p)\n",
    "print(\"Estimated AR coefficients (beta_hat):\", beta_hat)\n",
    "print(\"Estimated noise variance (sigma^2):\", sigma2)\n",
    "\n",
    "#check for stationarity \n",
    "\n",
    "poly_coefs = np.concatenate(([1], -beta_hat))\n",
    "\n",
    "# Find roots\n",
    "roots = np.roots(poly_coefs)\n",
    "\n",
    "print(\"Roots of the characteristic polynomial:\", roots)\n",
    "print(\"Absolute values of roots:\", np.abs(roots))\n",
    "\n",
    "# Stationarity check\n",
    "if np.all(np.abs(roots) > 1):\n",
    "    print(\"The AR process is stationary\")\n",
    "else:\n",
    "    print(\"The AR process is NOT stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MA(q) ; moving average\n",
    "\n",
    "An autoregressive process of order q, AR(q), models a time series  $X_t$ - average X,  as a linear function of its past values shocks:\n",
    "\n",
    "$$ X_t - \\mu_t  =  \\varepsilon_t  + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q} , \\quad \\varepsilon_i \\sim \\text{i.i.d. } (0, \\sigma^2) $$\n",
    "\n",
    "Essentially this is a linear regression (although we cant use OLS due to the nature of the equation).\n",
    "In plane terms, the value of a normalised X at time t is equal to the shocks in the past q periods where the older the stock the less impact on time t's value.\n",
    "\n",
    "Useful fact any AR(p) model can be represented as a MA($\\infty$), using this the inverse of a MA(q) given some conditions can be represented as a AR($\\infty$):\n",
    "$$ MA(1), y_t = \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}$$\n",
    "in its AR form is:\n",
    "$$ \\varepsilon_t = \\sum_{j=0}^{\\infty} (\\theta)^j y_{t-j}, |\\theta| < 1$$\n",
    "for more complex models you may see complex numbers so we will use notation z terms are your q:\n",
    "$$ \\theta(z) = 1 + \\theta_1 z + \\theta_2 z^2 + \\dots + \\theta_q z^q $$\n",
    "roots of this polynomial must satisfy $|z| > 1$. also the sum changes as we cant directly use $\\theta$ to:\n",
    "$$ \\varepsilon_t = \\sum_{j=0}^{\\infty} (\\psi)^j y_{t-j}$$\n",
    "where: $ \\psi_j = -(\\theta_1 \\psi_{j-1} + \\theta_2 \\psi_{j-2} +\\dots + \\theta_{\\min{j,q}} \\psi_{j-q})$, $\\psi_0 =1$\n",
    "\n",
    "so: $ \\psi_1 = -\\theta_1 $\n",
    "\n",
    "$\\psi_2 = -(\\theta_1 \\psi_1 + \\theta_2) = {\\theta_1}^2 - \\theta_2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.63081410e-16 1.51996030e-12 5.12589897e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r9/p1cb21jx0gx9cpxjwsglndy80000gn/T/ipykernel_84322/2151775836.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u[t] = y[t] - mu - np.dot(thetas, u[t-q:t][::-1])\n",
      "/var/folders/r9/p1cb21jx0gx9cpxjwsglndy80000gn/T/ipykernel_84322/2151775836.py:15: RuntimeWarning: overflow encountered in square\n",
      "  ll = -0.5 * np.sum(np.log(2*np.pi*sigma**2) + (u**2)/(sigma**2))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "/var/folders/r9/p1cb21jx0gx9cpxjwsglndy80000gn/T/ipykernel_84322/2151775836.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u[t] = y[t] - mu - np.dot(thetas, u[t-q:t][::-1])\n",
      "/var/folders/r9/p1cb21jx0gx9cpxjwsglndy80000gn/T/ipykernel_84322/2151775836.py:15: RuntimeWarning: overflow encountered in square\n",
      "  ll = -0.5 * np.sum(np.log(2*np.pi*sigma**2) + (u**2)/(sigma**2))\n"
     ]
    }
   ],
   "source": [
    "q=1\n",
    "def ma_loglike(params, y, q):\n",
    "    mu = params[0]\n",
    "    thetas = params[1:1+q]\n",
    "    sigma = params[-1]\n",
    "\n",
    "    T = len(y)\n",
    "    u = np.zeros(T)\n",
    "\n",
    "    # recursively compute innovations\n",
    "    for t in range(q, T):\n",
    "        u[t] = y[t] - mu - np.dot(thetas, u[t-q:t][::-1])\n",
    "\n",
    "    # Gaussian likelihood\n",
    "    ll = -0.5 * np.sum(np.log(2*np.pi*sigma**2) + (u**2)/(sigma**2))\n",
    "    return -ll  \n",
    "def fit_ma(y, q):\n",
    "    init = np.zeros(q+2)  # μ, θ₁…θ_q, σ\n",
    "    init[-1] = np.std(y)  # initial σ\n",
    "    out = minimize(ma_loglike, init, args=(y, q))\n",
    "    return out.x\n",
    "theta_hat = fit_ma(X, q)\n",
    "print(\"Estimated MA coefficients (mean (~0), theta_hats, variance):\", theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining them:\n",
    "we substitute the equation for MA(q) with the error as the dependent term into AR(p) to get:\n",
    "$$ X_t = X_{t-1} \\beta_{1} + X_{2} \\beta_{t-2} + \\cdots + X_{t-p} \\beta_{p} \n",
    "+u_t + \\phi_1 u_{t-1} + \\phi_2 u_{t-2} + \\dots + \\phi_q X_{t-q} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
